import json
import random
import math
import os
from collections import Counter

# --- 1. íŒŒì¼ ê²½ë¡œ ì„¤ì • (ìˆ˜ì •) ---
INPUT_ENGLISH_PAIRS = "english_annotated_pairs.jsonl"
INPUT_ARTLANG_PAIRS = "artlang_annotated_pairs.jsonl"

# ë² ì´ìŠ¤ ì¶œë ¥ ë””ë ‰í„°ë¦¬ ì •ì˜
OUTPUT_DIR = "output/dataset"

# Train/Dev íŒŒì¼ (8ê°œ) - ìœ ì§€
OUTPUT_ENG_EXP_TRAIN = os.path.join(OUTPUT_DIR, "train_eng_explicit.jsonl")
OUTPUT_ENG_EXP_DEV = os.path.join(OUTPUT_DIR, "dev_eng_explicit.jsonl")
OUTPUT_ARLA_EXP_TRAIN = os.path.join(OUTPUT_DIR, "train_arla_explicit.jsonl")
OUTPUT_ARLA_EXP_DEV = os.path.join(OUTPUT_DIR, "dev_arla_explicit.jsonl")

OUTPUT_ENG_IMP_TRAIN = os.path.join(OUTPUT_DIR, "train_eng_implicit.jsonl")
OUTPUT_ENG_IMP_DEV = os.path.join(OUTPUT_DIR, "dev_eng_implicit.jsonl")
OUTPUT_ARLA_IMP_TRAIN = os.path.join(OUTPUT_DIR, "train_arla_implicit.jsonl")
OUTPUT_ARLA_IMP_DEV = os.path.join(OUTPUT_DIR, "dev_arla_implicit.jsonl")

# Test íŒŒì¼ (2ê°œë¡œ ë‹¨ìˆœí™”)
OUTPUT_TEST_ENG = os.path.join(OUTPUT_DIR, "test_eng.jsonl")  # --- (ìˆ˜ì •) íŒŒì¼ëª… ë³€ê²½ ---
OUTPUT_TEST_ARLA = os.path.join(OUTPUT_DIR, "test_arla.jsonl")  # --- (ìˆ˜ì •) íŒŒì¼ëª… ë³€ê²½ ---

# --- 2. ìŠ¤í‚¤ë§ˆ ì •ì˜  ---
SCHEMA_DEFS = {
    "ENG_EXPLICIT_FIELDS": [
        "id", "pair_id", "type", "prompt", "text", "label", "tokens", "spans", "tags", "order", "meta"
    ],
    "ENG_IMPLICIT_FIELDS": [
        "id", "type", "prompt", "text", "label", "meta"
    ],
    "ARLA_EXPLICIT_FIELDS": [
        "id", "type", "prompt", "text", "label", "meta"
    ],
    "ARLA_IMPLICIT_FIELDS": [
        "id", "type", "prompt", "text", "label", "meta"
    ],
}

# --- 3. ëª©í‘œ í¬ê¸° ë° ë¹„ìœ¨ ìƒìˆ˜  ---
TARGET_TOTAL_PAIRS = 2000
TRAIN_RATIO = 0.8
DEV_RATIO = 0.1
TEST_RATIO = 0.1
assert math.isclose(TRAIN_RATIO + DEV_RATIO + TEST_RATIO, 1.0), "Ratios must sum to 1.0"

# --- 4. í˜•íƒœ ë³€ì´ í™•ë¥   ---
REGULAR_PROB = 0.50
IRREG1_PROB = 0.30
IRREG2_PROB = 0.20
MORPH_DEVIATION_THRESHOLD = 0.05

# --- 5. ê¸°íƒ€ ì„¤ì •  ---
MIN_TOKENS = 6
MAX_TOKENS = 25

ENGLISH_PROMPT = "Rule: Subject-Verb-Object order (adverb optional, sentence-final). Example: The dog eats the bone."
ARLA_PROMPT = "Rule: Subject-Object-Verb order (adverb optional, sentence-final). Example: pleck li vode lu praz noyka"


# --- load_annotated_pairs í•¨ìˆ˜  ---
def load_annotated_pairs(filepath, is_english=False):
    """
    .jsonl íŒŒì¼ì„ ì½ì–´ 'ok'ì™€ 'violation' ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    is_english=Trueì¼ ë•Œ, 6-25 í† í° ê¸¸ì´ í•„í„°ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    is_english=Falseì¼ ë•Œ, í˜•íƒœ(plural_type) ë¹„ìœ¨ì„ ì§‘ê³„í•©ë‹ˆë‹¤.
    """
    pairs_ok = []
    pairs_violation = []
    skipped_count = 0
    morph_counter = Counter()

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                entry = json.loads(line)

                # ì˜ì–´ ê¸¸ì´ í•„í„°
                if is_english:
                    token_count = len(entry.get("tokens", []))
                    if not (MIN_TOKENS <= token_count <= MAX_TOKENS):
                        skipped_count += 1
                        continue

                # ì¸ê³µì–´ í˜•íƒœ ì§‘ê³„ (ì •ë¬¸ 'ok' ê¸°ì¤€)
                if not is_english and entry["label"] == "ok":
                    meta = entry.get("meta", {})
                    s_plural = meta.get("s", {}).get("plural_type")
                    o_plural = meta.get("o", {}).get("plural_type")
                    if s_plural: morph_counter[s_plural] += 1
                    if o_plural: morph_counter[o_plural] += 1

                if entry["label"] == "ok":
                    pairs_ok.append(entry)
                else:
                    pairs_violation.append(entry)

    except FileNotFoundError:
        print(f"Error: Input file not found: {filepath}")
        exit(1)

    if is_english and skipped_count > 0:
        print(f"  (Filtered out {skipped_count} English samples due to length constraints [6-25 tokens])")

    return pairs_ok, pairs_violation, morph_counter


# --- verify_morphology í•¨ìˆ˜  ---
def verify_morphology(morph_counter):
    """
    ë¡œë“œëœ ì¸ê³µì–´ ë°ì´í„°ì˜ í˜•íƒœ ë¹„ìœ¨ì´ ëª©í‘œì¹˜ì™€ ë§ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    print("\nVerifying ArLa morphology ratios (based on 'ok' samples)...")
    total = sum(morph_counter.values())
    if total == 0:
        print("  Warning: No morphology data found to verify.")
        return

    real_reg_ratio = morph_counter.get("regular", 0) / total
    real_irreg1_ratio = morph_counter.get("irreg1", 0) / total
    real_irreg2_ratio = morph_counter.get("irreg2", 0) / total

    print(f"  Target: REG={REGULAR_PROB:.2%} | IRREG1={IRREG1_PROB:.2%} | IRREG2={IRREG2_PROB:.2%}")
    print(
        f"  Actual: REG={real_reg_ratio:.2%} | IRREG1={real_irreg1_ratio:.2%} | IRREG2={real_irreg2_ratio:.2%} (N={total})")

    if abs(real_reg_ratio - REGULAR_PROB) > MORPH_DEVIATION_THRESHOLD:
        print(f"  WARNING: Regular ratio deviation > {MORPH_DEVIATION_THRESHOLD:.0%}")
    if abs(real_irreg1_ratio - IRREG1_PROB) > MORPH_DEVIATION_THRESHOLD:
        print(f"  WARNING: Irreg1 ratio deviation > {MORPH_DEVIATION_THRESHOLD:.0%}")


# --- write_dataset_files í•¨ìˆ˜  ---
def write_dataset_files(
        ok_pairs: list,
        vio_pairs: list,
        lang: str,
        num_train_pairs: int,
        num_dev_pairs: int
):
    """
    ì—­í• : ì „ë‹¬ëœ (ok, vio) í˜ì–´ ë¦¬ìŠ¤íŠ¸ (Train+Dev í’€)ë¥¼ ë°›ì•„ì„œ
          Train/Dev, Explicit/Implicit íŒŒì¼ 4ê°œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """

    # 0. ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸ ë° ìŠ¬ë¼ì´ì‹±
    total_needed = num_train_pairs + num_dev_pairs
    if len(ok_pairs) < total_needed or len(vio_pairs) < total_needed:
        print(
            f"Error: Not enough pairs for {lang} Train/Dev split. Needed {total_needed}, Found {len(ok_pairs)} ok / {len(vio_pairs)} vio")
        exit(1)

    # 1. Train / Dev ìŠ¤í”Œë¦¿ (ì „ë‹¬ëœ í’€ì—ì„œ ìŠ¬ë¼ì´ì‹±)
    train_ok = ok_pairs[:num_train_pairs]
    train_vio = vio_pairs[:num_train_pairs]

    dev_ok = ok_pairs[num_train_pairs: total_needed]
    dev_vio = vio_pairs[num_train_pairs: total_needed]

    # 2. íŒŒì¼ ê²½ë¡œ ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if lang == "eng":
        prompt = ENGLISH_PROMPT
        rule = "SVO_word_order"
        out_exp_train = OUTPUT_ENG_EXP_TRAIN
        out_exp_dev = OUTPUT_ENG_EXP_DEV
        out_imp_train = OUTPUT_ENG_IMP_TRAIN
        out_imp_dev = OUTPUT_ENG_IMP_DEV
    else:  # arla
        prompt = ARLA_PROMPT
        rule = "SOV_word_order"
        out_exp_train = OUTPUT_ARLA_EXP_TRAIN
        out_exp_dev = OUTPUT_ARLA_EXP_DEV
        out_imp_train = OUTPUT_ARLA_IMP_TRAIN
        out_imp_dev = OUTPUT_ARLA_IMP_DEV

    # 3. íŒŒì¼ ìƒì„± í•¨ìˆ˜ (ë‚´ë¶€ í—¬í¼)
    def build_file(out_path, ok_list, vio_list, is_explicit):
        dataset = []
        for entry in (ok_list + vio_list):
            new_entry = {}

            # 1. ê³µí†µ í•„ë“œ
            new_entry["id"] = entry["id"]
            if not is_explicit:
                new_entry["id"] = new_entry["id"].replace("_ok", "_imp").replace("_vi", "_imp")

            new_entry["type"] = "explicit" if is_explicit else "implicit"
            new_entry["prompt"] = prompt if is_explicit else ""
            new_entry["text"] = entry["text"]
            new_entry["label"] = entry["label"]

            # 2. ë©”íƒ€ë°ì´í„° (langë³„ ë¶„ê¸°)
            if lang == "eng":
                new_entry["meta"] = {
                    "rule": rule,
                    "language": "english",
                    "length": len(entry.get("tokens", [])),
                    "source": entry.get("meta", {}).get("source", "simplewiki")
                }
                if entry["label"] == "violation":
                    new_entry["meta"]["perturbation"] = "swap(O,V)"
            else:  # arla
                new_entry["meta"] = entry.get("meta", {}).copy()  # ì›ë³¸ ìƒì† (ë³µì‚¬)
                new_entry["meta"]["rule"] = rule

            # 3. Explicit ì „ìš© í•„ë“œ (Eng)
            if is_explicit and lang == "eng":
                new_entry["pair_id"] = entry.get("pair_id")
                new_entry["tokens"] = entry.get("tokens")
                new_entry["spans"] = entry.get("spans")
                new_entry["tags"] = entry.get("tags")
                new_entry["order"] = entry.get("order")

            dataset.append(new_entry)

        random.shuffle(dataset)
        with open(out_path, "w", encoding="utf-8") as f:
            for entry in dataset:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        print(f"Built {out_path} with {len(dataset)} samples.")

    # 4. 4ê°œ íŒŒì¼ ë¹Œë“œ
    print(f"\nBuilding {lang.upper()} Train/Dev files...")
    build_file(out_exp_train, train_ok, train_vio, is_explicit=True)
    build_file(out_exp_dev, dev_ok, dev_vio, is_explicit=True)
    build_file(out_imp_train, train_ok, train_vio, is_explicit=False)
    build_file(out_imp_dev, dev_ok, dev_vio, is_explicit=False)


# --- (ìˆ˜ì •) write_test_files í•¨ìˆ˜: ë‹¨ìˆœí™” ---
def write_test_files(
        eng_test_ok: list, eng_test_vio: list,
        arla_test_ok: list, arla_test_vio: list
):
    """
    ì—­í• : ì˜ì–´ í…ŒìŠ¤íŠ¸ ì…‹ê³¼ ì¸ê³µì–´ í…ŒìŠ¤íŠ¸ ì…‹ íŒŒì¼ 2ê°œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    ëª¨ë“  í…ŒìŠ¤íŠ¸ íŒŒì¼ì€ 'implicit' í¬ë§·ì…ë‹ˆë‹¤.
    """

    def _format_test_entry(entry, lang):
        """ Test ìƒ˜í”Œìš© 'implicit' í¬ë§·ì„ ìƒì„±í•˜ëŠ” ë‚´ë¶€ í—¬í¼ """
        meta = entry.get("meta", {}).copy()

        # ì–¸ì–´ ì •ë³´ ì„¤ì •
        if lang == "eng":
            meta["language"] = "english"
            id_suffix = "_test_eng"
        else:
            meta["language"] = meta.get("language", "artificial")
            meta["rule"] = "SOV_word_order"
            id_suffix = "_test_arla"

        return {
            "id": entry["id"].replace("_ok", id_suffix).replace("_vi", id_suffix),
            "type": "implicit",
            "prompt": "",
            "text": entry["text"],
            "label": entry["label"],
            "meta": meta
        }

    def build_single_test_file(out_path, ok_list, vio_list, lang):
        dataset = []
        for entry in (ok_list + vio_list):
            dataset.append(_format_test_entry(entry, lang))

        random.shuffle(dataset)
        with open(out_path, "w", encoding="utf-8") as f:
            for entry in dataset:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        print(f"Built {out_path} with {len(dataset)} samples.")

    print("\nBuilding Simplified TEST files (Eng and ArLa)...")

    # ì˜ì–´ í…ŒìŠ¤íŠ¸ ì…‹ (400 ìƒ˜í”Œ)
    build_single_test_file(OUTPUT_TEST_ENG, eng_test_ok, eng_test_vio, "eng")

    # ì¸ê³µì–´ í…ŒìŠ¤íŠ¸ ì…‹ (400 ìƒ˜í”Œ)
    build_single_test_file(OUTPUT_TEST_ARLA, arla_test_ok, arla_test_vio, "arla")


# --- main í•¨ìˆ˜ (ìˆ˜ì •) ---
def main():
    # ì¶œë ¥ ë””ë ‰í„°ë¦¬ ìƒì„± ë¡œì§
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    print(f"Ensuring output directory exists: {OUTPUT_DIR}")

    # 1. ì˜ì–´ ë¡œë“œ (ê¸¸ì´ í•„í„° ì ìš©)
    print("Loading annotated English pairs...")
    eng_ok, eng_vio, _ = load_annotated_pairs(INPUT_ENGLISH_PAIRS, is_english=True)
    print(f"Loaded {len(eng_ok)} 'ok' and {len(eng_vio)} 'violation' English samples (after filtering).")

    # 2. ì¸ê³µì–´ ë¡œë“œ (í˜•íƒœ ì§‘ê³„)
    print("Loading annotated Artificial Language pairs...")
    arla_ok_all, arla_vio_all, morph_counts = load_annotated_pairs(INPUT_ARTLANG_PAIRS, is_english=False)
    print(f"Loaded {len(arla_ok_all)} 'ok' and {len(arla_vio_all)} 'violation' ArLa samples.")

    # 3. ì¸ê³µì–´ í˜•íƒœ ë¹„ìœ¨ ê²€ì¦
    verify_morphology(morph_counts)

    # 4. ğŸš¨ OOD ìŠ¤í”Œë¦¿ ë¡œì§ ì œê±°
    # ì´ì œ OOD ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šê³  ì „ì²´ ArLa ë°ì´í„°ë¥¼ IID í’€ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
    arla_ok_iid_pool = arla_ok_all
    arla_vio_iid_pool = arla_vio_all

    print(f"ArLa IID Pool size (Total): {len(arla_ok_iid_pool)} ok, {len(arla_vio_iid_pool)} vio")

    # 5. ëª¨ë“  í’€ ì…”í”Œ
    random.shuffle(eng_ok)
    random.shuffle(eng_vio)
    random.shuffle(arla_ok_iid_pool)
    random.shuffle(arla_vio_iid_pool)

    # 6. ê³ ì • ê°œìˆ˜ ê³„ì‚° (TARGET_TOTAL_PAIRS ê¸°ë°˜)

    # í•„ìš”í•œ ìµœì†Œ ê°œìˆ˜ í™•ì¸
    if min(len(eng_ok), len(eng_vio)) < TARGET_TOTAL_PAIRS:
        print(
            f"Fatal Error: Not enough balanced English pairs. Needed {TARGET_TOTAL_PAIRS}, Found {min(len(eng_ok), len(eng_vio))}.")
        exit(1)
    if min(len(arla_ok_iid_pool), len(arla_vio_iid_pool)) < TARGET_TOTAL_PAIRS:
        print(
            f"Fatal Error: Not enough balanced ArLa IID pairs. Needed {TARGET_TOTAL_PAIRS}, Found {min(len(arla_ok_iid_pool), len(arla_vio_iid_pool))}.")
        exit(1)

    # --- ê³ ì • ê°œìˆ˜ ê³„ì‚° ---
    NUM_TOTAL = TARGET_TOTAL_PAIRS  # 2000
    NUM_TEST = int(NUM_TOTAL * TEST_RATIO)  # 200
    NUM_TRAIN_DEV = NUM_TOTAL - NUM_TEST  # 1800

    NUM_DEV = int(NUM_TRAIN_DEV * (DEV_RATIO / (TRAIN_RATIO + DEV_RATIO)))  # 200
    NUM_TRAIN = NUM_TRAIN_DEV - NUM_DEV  # 1600

    print(f"\n--- Final Fixed Dataset Counts ({NUM_TOTAL} pairs total per language) ---")
    print(f"  Train: {NUM_TRAIN} pairs ({NUM_TRAIN * 2} samples)")
    print(f"  Dev:   {NUM_DEV} pairs ({NUM_DEV * 2} samples)")
    print(f"  Test:  {NUM_TEST} pairs ({NUM_TEST * 2} samples)")

    # 7. ë°ì´í„° í’€ ìŠ¬ë¼ì´ì‹± (ê³ ì • ê°œìˆ˜ ì¶”ì¶œ ë° ë¶„í• )

    # 7-A. ì˜ì–´ (Eng) - NUM_TOTAL ë§Œí¼ë§Œ ì‚¬ìš©
    eng_used_ok = eng_ok[:NUM_TOTAL]
    eng_used_vio = eng_vio[:NUM_TOTAL]

    # Test í’€ (10% - NUM_TEST)
    eng_test_ok = eng_used_ok[:NUM_TEST]
    eng_test_vio = eng_used_vio[:NUM_TEST]

    # Train/Dev í’€ (90% - ë‚˜ë¨¸ì§€)
    eng_train_dev_ok = eng_used_ok[NUM_TEST:]
    eng_train_dev_vio = eng_used_vio[NUM_TEST:]

    # 7-B. ì¸ê³µì–´ (ArLa) IID - NUM_TOTAL ë§Œí¼ë§Œ ì‚¬ìš©
    arla_used_ok = arla_ok_iid_pool[:NUM_TOTAL]
    arla_used_vio = arla_vio_iid_pool[:NUM_TOTAL]

    # Test í’€ (10% - NUM_TEST)
    arla_test_ok = arla_used_ok[:NUM_TEST]
    arla_test_vio = arla_used_vio[:NUM_TEST]

    # Train/Dev í’€ (90% - ë‚˜ë¨¸ì§€)
    arla_train_dev_ok = arla_used_ok[NUM_TEST:]
    arla_train_dev_vio = arla_used_vio[NUM_TEST:]

    # 8. Train/Dev íŒŒì¼ ë¹Œë“œ ì‹¤í–‰ (90% ë°ì´í„° ì‚¬ìš©)
    write_dataset_files(
        eng_train_dev_ok, eng_train_dev_vio, "eng",
        num_train_pairs=NUM_TRAIN,
        num_dev_pairs=NUM_DEV
    )

    write_dataset_files(
        arla_train_dev_ok, arla_train_dev_vio, "arla",
        num_train_pairs=NUM_TRAIN,
        num_dev_pairs=NUM_DEV
    )

    # 9. Test íŒŒì¼ ë¹Œë“œ ì‹¤í–‰ (10% ë°ì´í„° ì‚¬ìš©)
    write_test_files(
        eng_test_ok=eng_test_ok,
        eng_test_vio=eng_test_vio,
        arla_test_ok=arla_test_ok,
        arla_test_vio=arla_test_vio
    )

    print("\nAll datasets built successfully!")


if __name__ == "__main__":
    main()