import random
import json
import math
from tqdm import tqdm
import os

# --- ì„¤ì • (Constants) ---
OUTPUT_JSONL = "artlang_annotated_pairs.jsonl"
TARGET_PAIRS = 4000
MIN_TOKENS = 6
MAX_TOKENS = 25

# --- ğŸ¯ 1. Zipf ë¶„í¬ ìƒìˆ˜ ---
ZIPF_ALPHA = 1.07

# --- ğŸ¯ 2. ë¬¸ë²• ê·œì¹™ ìƒìˆ˜ (Pro Final Fix) ---
PLURAL_NOUN_PROB = 0.3
# ğŸš¨ ì˜¤ë¥˜ ìˆ˜ì •: PLURAL_RULESì™€ PLURAL_WEIGHTS ë¶„ë¦¬
PLURAL_RULES = {"regular": "-ka", "irreg1": "-po", "irreg2": "-lee"}
PLURAL_WEIGHTS = {"regular": 0.7, "irreg1": 0.15, "irreg2": 0.15}

# í˜•ìš©ì‚¬ 1-6ê°œ (PPë¡œ ê¸¸ì´ í™•ì¥)
ADJ_COUNTS = [0, 1, 2, 3, 4, 5, 6]
ADJ_PROBS =  [0.10, 0.30, 0.25, 0.15, 0.10, 0.05, 0.05]

ADV_PROB = 0.6
ADJ_PLURAL_RULE = "-z"

# ì „ì¹˜ì‚¬êµ¬(PP) ë„ì… (í€˜ëƒ ë°©ë²•ë¡ : ê²©)
PREPOSITION_PROB = 0.6

# --- ì–´íœ˜ ì •ì˜ ---
NOUNS = {"pleck": "m", "vode": "f", "klim": "m", "zuna": "f", "brip": "m", "lorf": "f", "niff": "m", "glim": "f"}
ADJECTIVES = {"trois": {"m": "troise", "f": "troiso"}, "neim": {"m": "neime", "f": "neimo"},
              "peli": {"m": "pelie", "f": "pelio"}, "glok": {"m": "gloke", "f": "gloko"}}
ARTICLES = {"m": "li", "f": "lu"}
VERBS = ["klin", "nim", "yab", "praz", "flig", "droz"]
ADVERBS = ["noyka", "zayma", "dema", "vogo"]
PREPOSITIONS = ["er", "ko", "po", "in"]


# --- ğŸ¯ 1. Zipf ìƒ˜í”ŒëŸ¬ êµ¬í˜„ ---
class ZipfianSampler:
    def __init__(self, vocab: list, alpha: float = 1.07):
        self.vocab = vocab
        self.ranks = list(range(1, len(vocab) + 1))
        self.weights = [1 / (rank ** alpha) for rank in self.ranks]

    def sample(self) -> str:
        return random.choices(self.vocab, weights=self.weights, k=1)[0]


# --- í’ˆì‚¬ë³„ ìƒ˜í”ŒëŸ¬ ì´ˆê¸°í™” ---
NOUN_SAMPLER = ZipfianSampler(list(NOUNS.keys()), ZIPF_ALPHA)
ADJ_SAMPLER = ZipfianSampler(list(ADJECTIVES.keys()), ZIPF_ALPHA)
VERB_SAMPLER = ZipfianSampler(list(VERBS), ZIPF_ALPHA)
ADV_SAMPLER = ZipfianSampler(list(ADVERBS), ZIPF_ALPHA)
PREP_SAMPLER = ZipfianSampler(PREPOSITIONS, ZIPF_ALPHA)


# --- ğŸ¯ 2 & 3. NP ìƒì„±ê¸° (ì˜¤ë¥˜ ìˆ˜ì •: PLURAL_PROBS ê´€ë ¨) ---
def make_np():
    """
    ì—­í• : ì¸ê³µì–´ ëª…ì‚¬êµ¬(NP) ìƒì„± [N (Adj*1-6) Art]
    ìˆ˜ì •: PLURAL_PROBS ê´€ë ¨ TypeError í•´ê²°
    """
    # 1. ëª…ì‚¬ ì„ íƒ (Zipf)
    noun_base = NOUN_SAMPLER.sample()
    gender = NOUNS[noun_base]

    # 2. ë³µìˆ˜ ì ìš© (íŒŒë¼ë¯¸í„°í™”)
    is_plural = random.random() < PLURAL_NOUN_PROB
    noun = noun_base
    plural_type = None
    if is_plural:
        # ğŸš¨ ì˜¤ë¥˜ ìˆ˜ì • ë¶€ë¶„: PLURAL_WEIGHTS ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½
        plural_type = random.choices(
            list(PLURAL_WEIGHTS.keys()),
            weights=list(PLURAL_WEIGHTS.values()),
            k=1
        )[0]
        noun += PLURAL_RULES[plural_type]

    np_tokens = [noun]
    adj_stems = []

    # 3. í˜•ìš©ì‚¬ ì¶”ê°€ (1-6ê°œ, ê°€ì¤‘ì¹˜ ì ìš©, ë¬¸ë²• ì¼ê´€ì„± ìˆ˜ì •)
    num_adjectives = random.choices(ADJ_COUNTS, weights=ADJ_PROBS, k=1)[0]

    for _ in range(num_adjectives):
        adj_stem = ADJ_SAMPLER.sample()

        # 3a. ì„±(Gender) ì¼ì¹˜
        adj_token = ADJECTIVES[adj_stem][gender]

        # 3b. ìˆ˜(Number) ì¼ì¹˜
        if is_plural:
            adj_token += ADJ_PLURAL_RULE

        np_tokens.append(adj_token)
        adj_stems.append(adj_stem)

    # 4. ê´€ì‚¬ ì¶”ê°€ (í•„ìˆ˜)
    np_tokens.append(ARTICLES[gender])

    meta = {
        "base": noun_base,
        "gender": gender,
        "is_plural": is_plural,
        "plural_type": plural_type,
        "adj": adj_stems
    }
    return np_tokens, meta


# --- ì „ì¹˜ì‚¬êµ¬(PP) ìƒì„±ê¸° ---
def make_pp():
    """
    ì—­í• : ì „ì¹˜ì‚¬êµ¬(PP) ìƒì„± [Prep NP]
    """
    prep_token = [PREP_SAMPLER.sample()]

    np_tokens, meta_np = make_np()

    pp_tokens = prep_token + np_tokens

    meta = {"prep": prep_token[0], "np": meta_np}
    return pp_tokens, meta


# --- ë¬¸ì¥ ìƒì„±ê¸° ( PP ì¶”ê°€) ---
def make_sentence_pair():
    """
    ì—­í• : SOV(ok)ì™€ SVO(violation) ë¬¸ì¥ ìŒ ìƒì„±
    ìˆ˜ì •: 40% í™•ë¥ ë¡œ PP(ì „ì¹˜ì‚¬êµ¬)ë¥¼ ë™ì‚¬ ì•ì— ì¶”ê°€
    """
    # 1. ì£¼ì–´(S), ëª©ì ì–´(O) ìƒì„±
    np_s_tokens, meta_s = make_np()
    while True:
        np_o_tokens, meta_o = make_np()
        if meta_s["base"] != meta_o["base"]:
            break

    # 2. ë™ì‚¬(V) ìƒì„± (Zipf)
    verb_token = [VERB_SAMPLER.sample()]

    # 3. ë¶€ì‚¬(ADV) ìƒì„±
    adv_token = []
    meta_adv = None
    if random.random() < ADV_PROB:
        adv_token = [ADV_SAMPLER.sample()]
        meta_adv = adv_token[0]

    # 4. ì „ì¹˜ì‚¬êµ¬(PP) ìƒì„±
    pp_token = []
    meta_pp = None
    if random.random() < PREPOSITION_PROB:
        pp_token, meta_pp = make_pp()

    # 5. 'ok' (SOV) ë¬¸ì¥ ìƒì„±
    # êµ¬ì¡°: [NP_S] [NP_O] [PP] [V] [ADV] -> ìµœëŒ€ 27í† í°ê¹Œì§€ ìƒì„± ê°€ëŠ¥ (25í† í° ëª©í‘œ ì¶©ì¡±)
    ok_tokens = np_s_tokens + np_o_tokens + pp_token + verb_token + adv_token
    ok_text = " ".join(ok_tokens)
    ok_meta = {
        "structure": "S-O-PP-V-ADV" if pp_token and adv_token else (
            "S-O-PP-V" if pp_token else ("S-O-V-ADV" if adv_token else "S-O-V")),
        "s": meta_s, "o": meta_o, "pp": meta_pp, "adv": meta_adv
    }

    # 6. 'violation' (SVO) ë¬¸ì¥ ìƒì„±
    # êµ¬ì¡°: [NP_S] [V] [NP_O] [PP] [ADV]
    vio_tokens = np_s_tokens + verb_token + np_o_tokens + pp_token + adv_token
    vio_text = " ".join(vio_tokens)
    vio_meta = {
        "structure": "S-V-O-PP-ADV" if pp_token and adv_token else (
            "S-V-O-PP" if pp_token else ("S-V-O-ADV" if adv_token else "S-V-O")),
        "s": meta_s, "o": meta_o, "pp": meta_pp, "adv": meta_adv
    }

    return (ok_text, ok_meta, ok_tokens), (vio_text, vio_meta, vio_tokens)


# --- íŒŒì¼ ë¡œë“œ (get_existing_pair_count) ---
def get_existing_pair_count(filepath: str) -> int:
    if not os.path.exists(filepath):
        return 0
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            line_count = sum(1 for _ in f)
        return line_count // 2
    except Exception as e:
        print(f"Warning: Could not read existing file {filepath}. Overwriting. Error: {e}")
        return 0


# --- ë©”ì¸ í•¨ìˆ˜ (main) ---
def main():
    existing_pairs = get_existing_pair_count(OUTPUT_JSONL)
    pairs_to_generate = TARGET_PAIRS - existing_pairs

    if pairs_to_generate <= 0:
        print(f"Target of {TARGET_PAIRS} pairs already met or exceeded.")
        return

    print(f"Found {existing_pairs} existing pairs. Generating {pairs_to_generate} new pairs...")

    with open(OUTPUT_JSONL, "a", encoding="utf-8") as f_out:
        pbar = tqdm(desc="Generating Artificial Language (SOV) pairs", total=pairs_to_generate)

        generated_count = 0
        while generated_count < pairs_to_generate:
            (ok_text, ok_meta, ok_tokens), (vio_text, vio_meta, vio_tokens) = make_sentence_pair()

            # MAX_TOKENS=25 í•„í„°ê°€ ì‘ë™í•˜ì—¬ 23~25í† í° ë¬¸ì¥ì„ ì•ˆì •ì ìœ¼ë¡œ í™•ë³´í•©ë‹ˆë‹¤.
            if not (MIN_TOKENS <= len(ok_tokens) <= MAX_TOKENS):
                continue

            pair_id_num = existing_pairs + generated_count + 1
            pair_id = f"art_{pair_id_num:06d}"

            ok_entry = {
                "id": f"{pair_id}_ok", "pair_id": pair_id, "text": ok_text, "label": "ok",
                "meta": {**ok_meta, "length": len(ok_tokens), "language": "artificial"}
            }
            vio_entry = {
                "id": f"{pair_id}_vi", "pair_id": pair_id, "text": vio_text, "label": "violation",
                "meta": {**vio_meta, "length": len(vio_tokens), "language": "artificial"}
            }

            f_out.write(json.dumps(ok_entry, ensure_ascii=False) + "\n")
            f_out.write(json.dumps(vio_entry, ensure_ascii=False) + "\n")
            f_out.flush()

            generated_count += 1
            pbar.update(1)

        pbar.close()

    print(f"Generation complete. Total pairs in file: {existing_pairs + generated_count}")


if __name__ == "__main__":
    main()