import torch
import json
import math
import numpy as np
import matplotlib.pyplot as plt
import argparse
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.nn import functional as F
from sklearn.metrics import roc_auc_score, roc_curve

# ==============================================================================
# [1] í”„ë¡¬í”„íŠ¸ ì •ì˜ (ì‚¬ìš©ì ì œê³µ ì½”ë“œ)
# ==============================================================================
EXPLICIT_RULE_CARD = """[GRAMMAR RULES]
1) Word Order: Subject-Verb-Object (SVO)
   - The subject comes first
   - The verb comes second  
   - The object comes third
   - Adverbs can appear optionally at the end

2) Examples:
   âœ“ Correct: "The dog eats the bone."
   âœ“ Correct: "They will hunt birds sometimes."
   âœ— Incorrect: "Eats the dog the bone." (VSO order)
   âœ— Incorrect: "The bone the dog eats." (OSV order)
"""
IMPLICIT_EXAMPLES = """[EXAMPLES]
âœ“ The dog eats the bone.
âœ“ They will hunt birds sometimes.
âœ“ Each zebra has a unique pattern.
âœ— Eats the dog the bone.
âœ— The bone the dog eats.
"""

CONDITION = "explicit" #"explicit" || "implicit"
DATA_PATH = "data/test_eng.jsonl"
MODEL_PATH = f"logs/{CONDITION}_gpt2/final_model"


def build_prompt(
        ex: dict,
        condition: str = "implicit",
        for_eval: bool = False,
        task_type: str = "generation",
) -> str:
    if for_eval:
        return ex.get("text", "")

    sent = ex.get("text", "")
    condition = (condition or "").lower()

    # í•™ìŠµ ì½”ë“œì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë¡œì§
    if condition == "explicit":
        rule_section = f"{EXPLICIT_RULE_CARD}\n\n"
    else:
        # Implicit ëª¨ë¸ í•™ìŠµ ë°©ì‹ì— ë”°ë¼ ìˆ˜ì • (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œ í¬í•¨)
        rule_section = f"{IMPLICIT_EXAMPLES}\n\n"
        # ë§Œì•½ Implicitì€ ì˜ˆì‹œ ì—†ì´ ë¬¸ì¥ë§Œ í•™ìŠµí–ˆë‹¤ë©´ ì•„ë˜ ì¤„ ì‚¬ìš©:
        # rule_section = ""

    if task_type == "generation":
        # PPL ê³„ì‚°ìš© (Sentence: ë’¤ì— ë¬¸ì¥ì´ ì˜´)
        prompt = f"{rule_section}Sentence: {sent}"

    elif task_type == "grammaticality":
        # Calibration ì¸¡ì •ìš© (Yes/No ì§ˆë¬¸)
        # ì£¼ì˜: ì´ í¬ë§·ì€ í•™ìŠµë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŒ (Zero-shot)
        prompt = (
            f"{rule_section}"
            f"Judge whether the following sentence is grammatically correct.\n"
            f"Sentence: {sent}\n"
            f"Answer (Yes/No):"
        )
    else:
        prompt = f"{rule_section}Sentence: {sent}"

    return prompt


# ==============================================================================
# [3] ì§€í‘œ ê³„ì‚° í´ë˜ìŠ¤
# ==============================================================================
class MetricCalculator:
    def __init__(self, model_path, condition="explicit"):
        print(f"Loading model from {model_path}...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            self.model = AutoModelForCausalLM.from_pretrained(model_path)
        except OSError:
            raise OSError(f"ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\nMODEL_PATH ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        self.model.eval()
        self.condition = condition

    def get_sentence_score(self, text):
        """
        ë¬¸ì¥ì˜ Log-Likelihood (LL) ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì ìˆ˜ê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡(ìŒìˆ˜ ê°’ì´ í´ìˆ˜ë¡) ëª¨ë¸ì´ ìì—°ìŠ¤ëŸ½ê²Œ ëŠë¼ëŠ” ë¬¸ì¥ì…ë‹ˆë‹¤.
        """
        # Generation íƒœìŠ¤í¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_prompt({"text": text}, condition=self.condition, task_type="generation")

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            # labelsë¥¼ ì£¼ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ Lossë¥¼ ê³„ì‚°
            outputs = self.model(**inputs, labels=inputs["input_ids"])

        loss = outputs.loss.item()
        # LL = -Loss * Length
        return -loss * inputs["input_ids"].shape[1]

    def get_calibration_prob(self, text):
        """
        Yes/No ë¬¸ì œì— ëŒ€í•œ ëª¨ë¸ì˜ í™•ì‹ ë„(Confidence)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        # Grammaticality íƒœìŠ¤í¬ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_prompt({"text": text}, condition=self.condition, task_type="grammaticality")

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(**inputs)
            # ë§ˆì§€ë§‰ í† í°(Yes/Noë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ìœ„ì¹˜)ì˜ Logits
            logits = outputs.logits[:, -1, :]

        probs = F.softmax(logits, dim=-1)

        # 'Yes'ì™€ 'No' í† í° ID ì°¾ê¸°
        yes_candidates = ["Yes", " Yes", "yes", " yes"]
        no_candidates = ["No", " No", "no", " no"]

        # í† í¬ë‚˜ì´ì €ì— ìˆëŠ” ì²« ë²ˆì§¸ í›„ë³´ í† í° ID ì‚¬ìš©
        yes_id = self.tokenizer.convert_tokens_to_ids(yes_candidates[0])
        no_id = self.tokenizer.convert_tokens_to_ids(no_candidates[0])

        prob_yes = probs[0, yes_id].item()
        prob_no = probs[0, no_id].item()

        # ì •ê·œí™”
        total = prob_yes + prob_no + 1e-12
        prob_yes /= total
        prob_no /= total

        # ì˜ˆì¸¡ ë° í™•ì‹ ë„
        pred_label = "ok" if prob_yes >= 0.5 else "violation"
        confidence = prob_yes if pred_label == "ok" else prob_no

        return pred_label, confidence


# ==============================================================================
# [4] ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ==============================================================================
def main():
    print(f"=== Configuration ===")
    print(f"Model Path: {MODEL_PATH}")
    print(f"Data Path : {DATA_PATH}")
    print(f"Condition : {CONDITION}")
    print(f"=====================\n")

    calculator = MetricCalculator(MODEL_PATH, condition=CONDITION)

    ok_scores = []
    viol_scores = []
    data_ok = []
    data_viol = []
    calib_data = []

    print(f"Running evaluation...")

    with open(DATA_PATH, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in tqdm(lines):
        if not line.strip(): continue
        try:
            ex = json.loads(line)
        except:
            continue

        text = ex.get("text")
        label = ex.get("label")  # "ok" or "violation"

        if not text or not label: continue

        score = calculator.get_sentence_score(text)

        if label == "ok":
            ok_scores.append(score)
        elif label == "violation":
            viol_scores.append(score)
        pred_label, conf = calculator.get_calibration_prob(text)
        is_correct = (pred_label == label)
        calib_data.append((is_correct, conf))


    # ==========================================================================
    # [Metric 1] AUC (ì •í™•ë„ ëŒ€ì²´ ì§€í‘œ)
    # ==========================================================================
    y_ture = [1] * len(ok_scores) + [0] * len(viol_scores)
    y_scores = ok_scores + viol_scores

    auc = roc_auc_score(y_ture, y_scores)
    print("\n" + "=" * 40)
    print(f" [Metric 1] AUC (Separability)")
    print("=" * 40)
    print(f" OK Samples      : {len(ok_scores)}")
    print(f" Viol Samples    : {len(viol_scores)}")
    print(f" â­ AUC Score     : {auc:.4f}")


    # ==========================================================================
    # [Metric 2] PLL Gap (ìì—°ìŠ¤ëŸ¬ì›€ ì ìˆ˜ ì°¨ì´)
    # ==========================================================================
    avg_ok = np.mean(ok_scores) if ok_scores else 0
    avg_viol = np.mean(viol_scores) if viol_scores else 0
    pll_gap = avg_ok - avg_viol

    print("\n" + "=" * 40)
    print(f" [Metric 2] PLL Gap Analysis ({CONDITION})")
    print("=" * 40)
    print(f" Average LL (OK)       : {avg_ok:.4f} (Higher is better)")
    print(f" Average LL (Violation): {avg_viol:.4f}")
    print(f" ----------------------------------------")
    print(f" â­ PLL Gap           : {pll_gap:.4f}")

    if pll_gap > 0:
        print(" âœ… Result: Success (ëª¨ë¸ì´ ì •ë¬¸ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ ëŠë‚Œ)")
    else:
        print(" âŒ Result: Fail (ëª¨ë¸ì´ ë¹„ë¬¸ì„ ë” ì„ í˜¸í•¨)")

    # ==========================================================================
    # [Metric 3] Calibration (ECE)
    # ==========================================================================
    confidences = np.array([x[1] for x in calib_data])
    corrects = np.array([x[0] for x in calib_data])
    accuracy = np.mean(corrects)

    n_bins = 10
    bin_boundaries = np.linspace(0, 1, n_bins + 1)
    ece = 0.0
    bin_accuracies = []
    bin_confs = []

    for i in range(n_bins):
        bin_mask = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])
        if np.sum(bin_mask) > 0:
            bin_acc = np.mean(corrects[bin_mask])
            bin_conf = np.mean(confidences[bin_mask])
            bin_accuracies.append(bin_acc)
            bin_confs.append(bin_conf)
            bin_weight = np.sum(bin_mask) / len(confidences)
            ece += bin_weight * np.abs(bin_acc - bin_conf)

    print("\n" + "=" * 40)
    print(f" [Metric 3] Calibration Analysis")
    print("=" * 40)
    print(f" Accuracy (Yes/No Task): {accuracy * 100:.2f}%")
    print(f" â­ ECE                 : {ece:.4f} (Lower is better)")

    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(6, 6))
    plt.plot([0, 1], [0, 1], linestyle="--", color="gray", label="Perfect Calibration")
    plt.plot(bin_confs, bin_accuracies, marker="o", color="blue", label=f"Model (ECE={ece:.2f})")
    plt.xlabel("Confidence")
    plt.ylabel("Accuracy")
    plt.title(f"Reliability Diagram ({CONDITION})")
    plt.legend()
    plt.grid(True, alpha=0.3)

    save_path = f"calibration_plot_{CONDITION}.png"
    plt.savefig(save_path)
    print(f" ğŸ“Š ê·¸ë˜í”„ ì €ì¥ë¨: {save_path}")


if __name__ == "__main__":
    main()